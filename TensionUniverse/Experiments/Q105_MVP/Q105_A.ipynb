{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD99fSCeRNrW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Basic configuration\n",
        "# -------------------------------\n",
        "\n",
        "print(\"TU Q105_A - Toy systemic crash warnings\")\n",
        "print(\"----------------------------------------\")\n",
        "print(\"This notebook simulates many tiny network contagion scenarios\")\n",
        "print(\"and computes a scalar tension observable T_warning for each\")\n",
        "print(\"early-warning scheme.\\n\")\n",
        "print(\"All runs are fully offline. No API key is needed.\\n\")\n",
        "\n",
        "RNG_SEED = 105\n",
        "N_NODES = 20\n",
        "N_CORE = 5\n",
        "N_RUNS = 500\n",
        "\n",
        "ALPHA_REDISPATCH = 0.8  # fraction of failed node load redistributed to neighbors\n",
        "BASE_CAPACITY = 1.0\n",
        "BASE_LOAD_MEAN = 0.6\n",
        "BASE_LOAD_STD = 0.05\n",
        "NEAR_THRESHOLD_RATIO = 0.8\n",
        "CRASH_FRACTION_THRESHOLD = 0.4  # systemic crash if >= 40% of nodes fail\n",
        "\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "\n",
        "print(\"Configured world:\")\n",
        "print(f\"- Nodes: {N_NODES} (core: {N_CORE}, periphery: {N_NODES - N_CORE})\")\n",
        "print(\"- Network: core-periphery with additional ring edges in the periphery\")\n",
        "print(f\"- Crash definition: cascade size >= {CRASH_FRACTION_THRESHOLD * 100:.0f}% of nodes\")\n",
        "print(f\"- Number of scenarios per scheme: {N_RUNS}\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Network construction\n",
        "# -------------------------------\n",
        "\n",
        "def build_core_periphery_network(n_nodes: int, n_core: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a simple weighted adjacency matrix for a core-periphery network.\n",
        "\n",
        "    Core nodes (0..n_core-1) form a dense subgraph.\n",
        "    Periphery nodes connect to one core node and to neighbors in a ring.\n",
        "    We use symmetric weights normalized per node.\n",
        "    \"\"\"\n",
        "    adj = np.zeros((n_nodes, n_nodes), dtype=float)\n",
        "\n",
        "    # Core: fully connected (no self loops)\n",
        "    for i in range(n_core):\n",
        "        for j in range(n_core):\n",
        "            if i != j:\n",
        "                adj[i, j] = 1.0\n",
        "\n",
        "    # Periphery: each connects to one core node and ring neighbors\n",
        "    for i in range(n_core, n_nodes):\n",
        "        # connect to a core node (round-robin)\n",
        "        core_target = i % n_core\n",
        "        adj[i, core_target] = 1.0\n",
        "        adj[core_target, i] = 1.0\n",
        "\n",
        "        # ring neighbors in periphery\n",
        "        if i < n_nodes - 1:\n",
        "            adj[i, i + 1] = 1.0\n",
        "            adj[i + 1, i] = 1.0\n",
        "        else:\n",
        "            # last node connects back to first periphery node\n",
        "            adj[i, n_core] = 1.0\n",
        "            adj[n_core, i] = 1.0\n",
        "\n",
        "    # Normalize outgoing weights per node so that sum_j w_ij = 1 where degree > 0\n",
        "    row_sums = adj.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0.0] = 1.0\n",
        "    adj = adj / row_sums\n",
        "\n",
        "    return adj\n",
        "\n",
        "\n",
        "ADJ_MATRIX = build_core_periphery_network(N_NODES, N_CORE)\n",
        "DEGREE = ADJ_MATRIX.sum(axis=1)  # should be 1.0 for nodes with neighbors\n",
        "CORE_INDEX = np.arange(0, N_CORE)\n",
        "PERIPHERY_INDEX = np.arange(N_CORE, N_NODES)\n",
        "\n",
        "print(\"Network built.\")\n",
        "print(f\"- Average degree (by weight normalisation): {DEGREE.mean():.2f}\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 3. State initialisation helpers\n",
        "# -------------------------------\n",
        "\n",
        "@dataclass\n",
        "class WorldState:\n",
        "    capacities: np.ndarray  # shape (N_NODES,)\n",
        "    base_loads: np.ndarray  # shape (N_NODES,)\n",
        "\n",
        "\n",
        "def sample_world_state() -> WorldState:\n",
        "    \"\"\"\n",
        "    Sample node capacities and baseline loads.\n",
        "\n",
        "    Capacities are around BASE_CAPACITY with small heterogeneity.\n",
        "    Baseline loads are below capacity on average.\n",
        "    \"\"\"\n",
        "    capacities = np.random.normal(loc=BASE_CAPACITY, scale=0.05, size=N_NODES)\n",
        "    capacities = np.clip(capacities, 0.8, 1.2)\n",
        "\n",
        "    loads = np.random.normal(loc=BASE_LOAD_MEAN, scale=BASE_LOAD_STD, size=N_NODES)\n",
        "    loads = np.clip(loads, 0.3, 0.9)\n",
        "\n",
        "    return WorldState(capacities=capacities, base_loads=loads)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Contagion dynamics\n",
        "# -------------------------------\n",
        "\n",
        "def run_cascade(world: WorldState, shock_nodes: List[int], shock_size: float) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run a cascade process on a copy of the world state.\n",
        "\n",
        "    Returns:\n",
        "        failed_mask: boolean array, True where node ever failed.\n",
        "        final_loads: final load vector after cascade.\n",
        "    \"\"\"\n",
        "    capacities = world.capacities.copy()\n",
        "    loads = world.base_loads.copy()\n",
        "    n = len(loads)\n",
        "\n",
        "    # apply shock\n",
        "    for node in shock_nodes:\n",
        "        loads[node] += shock_size\n",
        "\n",
        "    failed = np.zeros(n, dtype=bool)\n",
        "\n",
        "    # iterative failure propagation\n",
        "    while True:\n",
        "        # nodes that newly fail in this step\n",
        "        to_fail = (~failed) & (loads > capacities)\n",
        "        if not np.any(to_fail):\n",
        "            break\n",
        "\n",
        "        # compute redistributed load from failures\n",
        "        outgoing_load = loads[to_fail].copy()\n",
        "        loads[to_fail] = 0.0\n",
        "        failed[to_fail] = True\n",
        "\n",
        "        if np.any(outgoing_load):\n",
        "            # total load to redistribute from all failing nodes\n",
        "            # distribute alpha fraction according to adjacency\n",
        "            load_vec = np.zeros(n, dtype=float)\n",
        "            load_vec[to_fail] = outgoing_load\n",
        "            redistributed = ALPHA_REDISPATCH * (load_vec @ ADJ_MATRIX)\n",
        "            loads += redistributed\n",
        "\n",
        "    return failed, loads\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Indicator computation\n",
        "# -------------------------------\n",
        "\n",
        "def compute_indicators(world: WorldState) -> Dict[str, float]:\n",
        "    \"\"\"Compute pre-shock indicators on the baseline state.\"\"\"\n",
        "    ratios = world.base_loads / world.capacities\n",
        "    mean_ratio = float(np.mean(ratios))\n",
        "    tail_fraction = float(np.mean(ratios > NEAR_THRESHOLD_RATIO))\n",
        "    core_ratio = float(np.mean(ratios[CORE_INDEX] / world.capacities[CORE_INDEX]))\n",
        "    # simple centrality proxy: degree-weighted ratio\n",
        "    centrality_weights = DEGREE / DEGREE.sum()\n",
        "    centrality_weighted_ratio = float(np.sum(centrality_weights * ratios))\n",
        "\n",
        "    return {\n",
        "        \"mean_ratio\": mean_ratio,\n",
        "        \"tail_fraction\": tail_fraction,\n",
        "        \"core_ratio\": core_ratio,\n",
        "        \"centrality_weighted_ratio\": centrality_weighted_ratio,\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Early-warning schemes\n",
        "# -------------------------------\n",
        "\n",
        "def scheme_global_mean(indicators: Dict[str, float]) -> bool:\n",
        "    \"\"\"\n",
        "    Global mean scheme:\n",
        "    - Warn when the global mean ratio is high.\n",
        "    \"\"\"\n",
        "    return indicators[\"mean_ratio\"] > 0.70\n",
        "\n",
        "\n",
        "def scheme_tail_sensitive(indicators: Dict[str, float]) -> bool:\n",
        "    \"\"\"\n",
        "    Tail-sensitive scheme:\n",
        "    - Warn when many nodes are near threshold, or the mean is very high.\n",
        "    \"\"\"\n",
        "    return (indicators[\"tail_fraction\"] > 0.25) or (indicators[\"mean_ratio\"] > 0.65)\n",
        "\n",
        "\n",
        "def scheme_core_focused(indicators: Dict[str, float]) -> bool:\n",
        "    \"\"\"\n",
        "    Core-focused scheme:\n",
        "    - Warn when core nodes look stressed, or when high-centrality nodes\n",
        "      are collectively close to capacity.\n",
        "    \"\"\"\n",
        "    return (indicators[\"core_ratio\"] > 0.72) or (\n",
        "        indicators[\"centrality_weighted_ratio\"] > 0.70 and indicators[\"tail_fraction\"] > 0.15\n",
        "    )\n",
        "\n",
        "\n",
        "SCHEMES = {\n",
        "    \"global_mean\": scheme_global_mean,\n",
        "    \"tail_sensitive\": scheme_tail_sensitive,\n",
        "    \"core_focused\": scheme_core_focused,\n",
        "}\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Scenario simulation\n",
        "# -------------------------------\n",
        "\n",
        "print(\"Simulating scenarios...\")\n",
        "records = []\n",
        "\n",
        "for run_idx in range(N_RUNS):\n",
        "    world = sample_world_state()\n",
        "    indicators = compute_indicators(world)\n",
        "\n",
        "    # simple shock model: hit one random node in the periphery with a moderate shock\n",
        "    # and with small probability also hit a core node\n",
        "    shock_nodes: List[int] = []\n",
        "    periphery_node = random.choice(list(PERIPHERY_INDEX))\n",
        "    shock_nodes.append(periphery_node)\n",
        "\n",
        "    if random.random() < 0.2:\n",
        "        core_node = random.choice(list(CORE_INDEX))\n",
        "        shock_nodes.append(core_node)\n",
        "\n",
        "    shock_size = random.uniform(0.3, 0.6)\n",
        "\n",
        "    failed_mask, final_loads = run_cascade(world, shock_nodes, shock_size)\n",
        "    cascade_size = float(np.mean(failed_mask))\n",
        "    crash = cascade_size >= CRASH_FRACTION_THRESHOLD\n",
        "\n",
        "    record = {\n",
        "        \"run\": run_idx,\n",
        "        \"crash\": int(crash),\n",
        "        \"cascade_size\": cascade_size,\n",
        "        \"shock_size\": shock_size,\n",
        "        \"n_shock_nodes\": len(shock_nodes),\n",
        "    }\n",
        "    record.update(indicators)\n",
        "    records.append(record)\n",
        "\n",
        "df = pd.DataFrame.from_records(records)\n",
        "base_crash_rate = df[\"crash\"].mean()\n",
        "\n",
        "print(\"Scenario simulation completed.\")\n",
        "print(f\"- Observed crash rate across all runs: {base_crash_rate:.3f}\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Evaluate schemes and compute T_warning\n",
        "# -------------------------------\n",
        "\n",
        "def evaluate_scheme(df_scenarios: pd.DataFrame, name: str, warn_fn) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate a single warning scheme and compute confusion matrix plus T_warning.\"\"\"\n",
        "    warn_flags = df_scenarios.apply(lambda row: warn_fn(row.to_dict()), axis=1).astype(int)\n",
        "    crash_flags = df_scenarios[\"crash\"].astype(int).values\n",
        "\n",
        "    TP = int(((warn_flags == 1) & (crash_flags == 1)).sum())\n",
        "    FP = int(((warn_flags == 1) & (crash_flags == 0)).sum())\n",
        "    FN = int(((warn_flags == 0) & (crash_flags == 1)).sum())\n",
        "    TN = int(((warn_flags == 0) & (crash_flags == 0)).sum())\n",
        "\n",
        "    total = TP + FP + FN + TN\n",
        "\n",
        "    FN_denom = TP + FN\n",
        "    FP_denom = FP + TN\n",
        "\n",
        "    FN_rate = FN / FN_denom if FN_denom > 0 else 0.0\n",
        "    FP_rate = FP / FP_denom if FP_denom > 0 else 0.0\n",
        "\n",
        "    warn_share = (TP + FP) / total if total > 0 else 0.0\n",
        "    crash_rate = (TP + FN) / total if total > 0 else 0.0\n",
        "\n",
        "    warn_crash_denom = TP + FP\n",
        "    quiet_crash_denom = FN + TN\n",
        "\n",
        "    warn_crash_rate = TP / warn_crash_denom if warn_crash_denom > 0 else 0.0\n",
        "    quiet_crash_rate = FN / quiet_crash_denom if quiet_crash_denom > 0 else 0.0\n",
        "\n",
        "    # tension components\n",
        "    T_FN = 3.0 * FN_rate\n",
        "    T_FP = 1.0 * FP_rate\n",
        "    T_cal = abs(warn_crash_rate - crash_rate)\n",
        "\n",
        "    T_warning = T_FN + T_FP + T_cal\n",
        "\n",
        "    return {\n",
        "        \"scheme_name\": name,\n",
        "        \"FN_rate\": FN_rate,\n",
        "        \"FP_rate\": FP_rate,\n",
        "        \"warn_share\": warn_share,\n",
        "        \"crash_rate\": crash_rate,\n",
        "        \"warn_crash_rate\": warn_crash_rate,\n",
        "        \"quiet_crash_rate\": quiet_crash_rate,\n",
        "        \"T_warning\": T_warning,\n",
        "        \"TP\": TP,\n",
        "        \"FP\": FP,\n",
        "        \"FN\": FN,\n",
        "        \"TN\": TN,\n",
        "    }\n",
        "\n",
        "\n",
        "summary_rows = []\n",
        "for name, fn in SCHEMES.items():\n",
        "    summary_rows.append(evaluate_scheme(df, name, fn))\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df = summary_df.sort_values(\"T_warning\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "print(\"Summary table (sorted by T_warning, lower means better-calibrated warnings):\")\n",
        "print(summary_df[[\n",
        "    \"scheme_name\",\n",
        "    \"FN_rate\",\n",
        "    \"FP_rate\",\n",
        "    \"warn_share\",\n",
        "    \"crash_rate\",\n",
        "    \"warn_crash_rate\",\n",
        "    \"quiet_crash_rate\",\n",
        "    \"T_warning\",\n",
        "]])\n",
        "print(\"\")\n",
        "\n",
        "print(\"Quick interpretation:\")\n",
        "for _, row in summary_df.iterrows():\n",
        "    name = row[\"scheme_name\"]\n",
        "    T = row[\"T_warning\"]\n",
        "    FN_rate = row[\"FN_rate\"]\n",
        "    FP_rate = row[\"FP_rate\"]\n",
        "    print(\n",
        "        f\"- {name}: T_warning ≈ {T:.3f}, \"\n",
        "        f\"FN_rate ≈ {FN_rate:.2f}, FP_rate ≈ {FP_rate:.2f}\"\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Plots\n",
        "# -------------------------------\n",
        "\n",
        "# 9.1 Crash probability vs mean_ratio (effective indicator view)\n",
        "\n",
        "bins = np.linspace(df[\"mean_ratio\"].min(), df[\"mean_ratio\"].max(), 8)\n",
        "bin_indices = np.digitize(df[\"mean_ratio\"].values, bins, right=True)\n",
        "\n",
        "bin_centers = []\n",
        "crash_probs = []\n",
        "counts = []\n",
        "\n",
        "for b in range(1, len(bins) + 1):\n",
        "    mask = bin_indices == b\n",
        "    if not np.any(mask):\n",
        "        continue\n",
        "    bin_centers.append(df[\"mean_ratio\"].values[mask].mean())\n",
        "    crash_probs.append(df[\"crash\"].values[mask].mean())\n",
        "    counts.append(mask.sum())\n",
        "\n",
        "plt.figure(figsize=(8, 4.5))\n",
        "plt.plot(bin_centers, crash_probs, marker=\"o\")\n",
        "plt.xlabel(\"Mean load / capacity ratio\")\n",
        "plt.ylabel(\"Crash probability in bin\")\n",
        "plt.title(\"TU Q105_A · Crash probability vs mean load ratio\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "indicator_plot_path = \"Q105A_indicator_vs_crash.png\"\n",
        "plt.tight_layout()\n",
        "plt.savefig(indicator_plot_path, dpi=150)\n",
        "plt.show()\n",
        "print(f\"Saved indicator vs crash plot as: {indicator_plot_path}\")\n",
        "\n",
        "# 9.2 Bar plot of T_warning per scheme\n",
        "\n",
        "plt.figure(figsize=(6, 4.5))\n",
        "plt.bar(summary_df[\"scheme_name\"], summary_df[\"T_warning\"])\n",
        "plt.xlabel(\"Early-warning scheme\")\n",
        "plt.ylabel(\"T_warning (higher = more tension)\")\n",
        "plt.title(\"TU Q105_A · T_warning per scheme\")\n",
        "plt.tight_layout()\n",
        "tension_bar_path = \"Q105A_T_warning.png\"\n",
        "plt.savefig(tension_bar_path, dpi=150)\n",
        "plt.show()\n",
        "print(f\"Saved T_warning bar plot as: {tension_bar_path}\")\n"
      ]
    }
  ]
}