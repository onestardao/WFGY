# ğŸ—„ï¸ Vector Store RAG Template Errors Â· Troubleshooting Guide
### Fixing data loading, ingestion, and retrieval errors in RAG templates

This guide addresses **Vector Store RAG template errors** â€” when you encounter issues loading data, ingesting documents, or retrieving results from RAG templates in LangFlow, LangChain, LlamaIndex, or similar frameworks.

---

## â“ Why Vector Store RAG Templates Fail

| Root Cause | What Happens in Practice | WFGY Lens |
|------------|-------------------------|-----------|
| **Dimension mismatch** | Embedding dimension doesn't match index | Pre-deploy collapse ([#16](./predeploy-collapse.md)) |
| **Missing API keys** | Vector store or embedding provider not configured | Bootstrap ordering ([#14](./bootstrap-ordering.md)) |
| **Empty index** | Query runs before data is loaded | Î”S = 0.0 â†’ no semantic content |
| **Metric mismatch** | Index uses cosine, query uses euclidean | [Semantic â‰  Embedding](./embedding-vs-semantic.md) (#5) |
| **Schema drift** | Document schema changed but index wasn't rebuilt | [Vectorstore Fragmentation](./vectorstore-fragmentation.md) |
| **Concurrent writes** | Multiple processes write to same index | [Deployment Deadlock](./deployment-deadlock.md) (#15) |

---

## ğŸ” Common Error Messages & Fixes

### Error: "Index not found" or "Collection does not exist"

**Symptom**:
```python
# Pinecone
pinecone.exceptions.NotFoundException: Index 'my-index' not found

# Chroma
ValueError: Collection 'langchain' does not exist

# FAISS
FileNotFoundError: [Errno 2] No such file or directory: 'faiss.index'
```

**Diagnosis**: Index wasn't created or wrong name used.  
**WFGY Module**: [Bootstrap Ordering](./bootstrap-ordering.md) (#14)

**Fix**:
```python
# âœ… Check if index exists before querying
import pinecone

pc = pinecone.Pinecone(api_key="YOUR_KEY")

# List all indexes
existing_indexes = pc.list_indexes()
print(f"Available indexes: {[i.name for i in existing_indexes]}")

# Create if missing
if "my-index" not in [i.name for i in existing_indexes]:
    pc.create_index(
        name="my-index",
        dimension=1536,  # Must match your embedding model
        metric="cosine"
    )
    print("âœ… Created index 'my-index'")
```

**WFGY Semantic Check**:
```python
# Add Î”S logging to detect empty index
def query_with_wfgy_check(index, query_embedding):
    results = index.query(vector=query_embedding, top_k=5)
    
    if len(results.matches) == 0:
        # Î”S = 0.0 â†’ no semantic content found
        print("âš ï¸  WFGY Alert: Î”S = 0.0 â€” Index is empty or query doesn't match")
        return None
    
    # Calculate Î”S from similarity scores
    avg_score = sum(m.score for m in results.matches) / len(results.matches)
    delta_s = 1.0 - avg_score  # Low delta_s = good match
    
    if delta_s > 0.45:
        print(f"âš ï¸  WFGY Alert: Î”S = {delta_s:.2f} â€” Weak semantic match")
    
    return results
```

---

### Error: "Dimension mismatch"

**Symptom**:
```python
# FAISS
ValueError: cannot reshape array of size 384 into shape (1536,)

# Pinecone
pinecone.exceptions.ApiException: Vector dimension 384 does not match index dimension 1536

# Chroma
ValueError: Embedding dimension 384 does not match collection dimension 1536
```

**Diagnosis**: Your embedding model produces vectors of different size than the index expects.  
**WFGY Module**: [Pre-Deploy Collapse](./predeploy-collapse.md) (#16)

**Common Embedding Dimensions**:
| Model | Dimension | Provider |
|-------|-----------|----------|
| `text-embedding-ada-002` | 1536 | OpenAI |
| `text-embedding-3-small` | 1536 | OpenAI |
| `text-embedding-3-large` | 3072 | OpenAI |
| `all-MiniLM-L6-v2` | 384 | Sentence-Transformers |
| `all-mpnet-base-v2` | 768 | Sentence-Transformers |
| `embed-english-v3.0` | 1024 | Cohere |
| `voyage-2` | 1024 | Voyage |

**Fix**:
```python
# âœ… Always verify dimension before creating index
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# Test embedding to get dimension
test_embedding = embeddings.embed_query("test")
dimension = len(test_embedding)
print(f"âœ… Embedding dimension: {dimension}")

# Create index with correct dimension
import pinecone

pc = pinecone.Pinecone(api_key="YOUR_KEY")
pc.create_index(
    name="my-index",
    dimension=dimension,  # Use detected dimension
    metric="cosine"
)
```

**WFGY Validation**:
```python
def validate_embedding_dimension(embeddings, index_dimension):
    """
    WFGY Pre-Deploy Check: Ensure embedding matches index.
    Run this BEFORE any insert/query operations.
    """
    test_vec = embeddings.embed_query("test dimension check")
    actual_dim = len(test_vec)
    
    if actual_dim != index_dimension:
        raise ValueError(
            f"âŒ WFGY Dimension Mismatch!\n"
            f"   Embedding model produces: {actual_dim}\n"
            f"   Index expects: {index_dimension}\n"
            f"   Fix: Recreate index with dimension={actual_dim}"
        )
    
    print(f"âœ… WFGY Dimension Check: {actual_dim} matches index")
    return True
```

---

### Error: "API Key not configured" or Authentication Failed

**Symptom**:
```python
# OpenAI
openai.error.AuthenticationError: No API key provided

# Pinecone
pinecone.exceptions.UnauthorizedException: Invalid API key

# Chroma (cloud)
chromadb.errors.AuthError: Authentication failed
```

**Diagnosis**: Missing or incorrect API keys.  
**WFGY Module**: [Bootstrap Ordering](./bootstrap-ordering.md) (#14)

**Fix**:
```python
import os

# âœ… Verify all required API keys at startup
def verify_api_keys():
    """
    WFGY Bootstrap Check: Verify all keys before starting pipeline.
    """
    required_keys = {
        'OPENAI_API_KEY': 'OpenAI (embeddings & LLM)',
        'PINECONE_API_KEY': 'Pinecone (vector store)',
        'PINECONE_ENVIRONMENT': 'Pinecone region',
    }
    
    missing = []
    for key, description in required_keys.items():
        value = os.getenv(key)
        if not value:
            missing.append(f"  âŒ {key}: {description}")
        else:
            # Mask key for security
            masked = value[:4] + "****" + value[-4:]
            print(f"  âœ… {key}: {masked}")
    
    if missing:
        print("\nâš ï¸  WFGY Bootstrap Failed â€” Missing keys:")
        for m in missing:
            print(m)
        raise EnvironmentError("Configure missing API keys before proceeding")
    
    print("\nâœ… WFGY Bootstrap: All API keys verified")
    return True


# Run at startup
verify_api_keys()
```

**Environment Setup**:
```bash
# .env file
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX=my-index

# LangFlow specific
LANGFLOW_STORE=pinecone
```

---

### Error: "Empty results" or "No documents found"

**Symptom**:
```python
# Returns empty list
results = vectorstore.similarity_search("my question")
print(results)  # []

# Or very low scores
# [Document(score=0.12, content="...")]
```

**Diagnosis**: Either data wasn't loaded, or query doesn't match semantically.  
**WFGY Module**: [Hallucination & Chunk Drift](./hallucination.md) (#1), [Semantic â‰  Embedding](./embedding-vs-semantic.md) (#5)

**Fix â€” Step 1: Verify Data Was Loaded**:
```python
def verify_data_loaded(vectorstore):
    """
    WFGY Check: Ensure index has data before querying.
    """
    # For FAISS
    if hasattr(vectorstore, 'index'):
        count = vectorstore.index.ntotal
        print(f"âœ… FAISS index contains {count} vectors")
        return count > 0
    
    # For Pinecone
    if hasattr(vectorstore, '_index'):
        stats = vectorstore._index.describe_index_stats()
        count = stats.total_vector_count
        print(f"âœ… Pinecone index contains {count} vectors")
        return count > 0
    
    # For Chroma
    if hasattr(vectorstore, '_collection'):
        count = vectorstore._collection.count()
        print(f"âœ… Chroma collection contains {count} documents")
        return count > 0
    
    print("âš ï¸  Unknown vectorstore type")
    return None
```

**Fix â€” Step 2: Debug Low Scores**:
```python
def debug_low_scores(vectorstore, query, threshold=0.5):
    """
    WFGY Diagnostic: Understand why scores are low.
    """
    results = vectorstore.similarity_search_with_score(query, k=10)
    
    print(f"\nğŸ” Query: '{query}'")
    print(f"ğŸ“Š Results (top 10):\n")
    
    for i, (doc, score) in enumerate(results):
        status = "âœ…" if score >= threshold else "âš ï¸ "
        preview = doc.page_content[:100].replace('\n', ' ')
        print(f"  {i+1}. {status} Score: {score:.3f} | {preview}...")
    
    avg_score = sum(s for _, s in results) / len(results) if results else 0
    delta_s = 1.0 - avg_score
    
    print(f"\nğŸ“ˆ WFGY Metrics:")
    print(f"   Average score: {avg_score:.3f}")
    print(f"   Î”S (semantic tension): {delta_s:.3f}")
    
    if delta_s > 0.45:
        print(f"   âš ï¸  High Î”S suggests:")
        print(f"      - Query language differs from document language")
        print(f"      - Documents weren't chunked well")
        print(f"      - Wrong embedding model used")
    else:
        print(f"   âœ… Î”S is healthy â€” retrieval working correctly")
```

---

### Error: "Rate limit exceeded" during bulk ingestion

**Symptom**:
```python
openai.error.RateLimitError: Rate limit reached for default-text-embedding-ada-002

# Or
pinecone.core.exceptions.ServiceException: 429 Too Many Requests
```

**Diagnosis**: Sending too many API calls too quickly.  
**WFGY Module**: [Deployment Deadlock](./deployment-deadlock.md) (#15)

**Fix**:
```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=2, max=60)
)
def embed_with_retry(texts, embeddings):
    """
    WFGY: Retry with exponential backoff.
    """
    return embeddings.embed_documents(texts)


def batch_ingest_with_rate_limit(
    documents: list,
    vectorstore,
    batch_size: int = 50,
    delay_seconds: float = 1.0
):
    """
    WFGY-compliant batch ingestion with rate limiting.
    """
    total = len(documents)
    
    for i in range(0, total, batch_size):
        batch = documents[i:i + batch_size]
        
        try:
            vectorstore.add_documents(batch)
            print(f"âœ… Ingested {min(i + batch_size, total)}/{total}")
        except Exception as e:
            print(f"âš ï¸  Batch {i//batch_size} failed: {e}")
            print(f"   Retrying with smaller batch...")
            
            # Retry with smaller batch
            for doc in batch:
                time.sleep(0.5)
                vectorstore.add_documents([doc])
        
        # Rate limit delay
        time.sleep(delay_seconds)
    
    print(f"\nâœ… Complete! Ingested {total} documents")
```

---

## ğŸ› ï¸ Complete RAG Template Debug Workflow

Run this diagnostic workflow when your RAG template isn't working:

```python
def wfgy_rag_diagnostic(
    vectorstore,
    embeddings,
    test_query: str = "test query",
    expected_dimension: int = 1536
):
    """
    Complete WFGY diagnostic for RAG template errors.
    Run this to identify exactly what's broken.
    """
    
    print("=" * 60)
    print("ğŸ¥ WFGY RAG Template Diagnostic")
    print("=" * 60)
    
    issues = []
    
    # Check 1: API Keys
    print("\nğŸ“‹ Check 1: API Keys")
    try:
        test_embed = embeddings.embed_query("test")
        print("   âœ… Embedding API working")
    except Exception as e:
        issues.append(f"Embedding API error: {e}")
        print(f"   âŒ Embedding API failed: {e}")
    
    # Check 2: Dimension
    print("\nğŸ“‹ Check 2: Embedding Dimension")
    try:
        test_embed = embeddings.embed_query("dimension check")
        actual_dim = len(test_embed)
        if actual_dim == expected_dimension:
            print(f"   âœ… Dimension correct: {actual_dim}")
        else:
            issues.append(f"Dimension mismatch: {actual_dim} vs {expected_dimension}")
            print(f"   âŒ Dimension mismatch: got {actual_dim}, expected {expected_dimension}")
    except Exception as e:
        issues.append(f"Dimension check failed: {e}")
    
    # Check 3: Index exists and has data
    print("\nğŸ“‹ Check 3: Vector Store Data")
    try:
        data_loaded = verify_data_loaded(vectorstore)
        if data_loaded:
            print("   âœ… Index has data")
        else:
            issues.append("Index is empty â€” no documents loaded")
            print("   âŒ Index is empty")
    except Exception as e:
        issues.append(f"Vector store check failed: {e}")
        print(f"   âŒ Vector store error: {e}")
    
    # Check 4: Query returns results
    print("\nğŸ“‹ Check 4: Query Test")
    try:
        results = vectorstore.similarity_search_with_score(test_query, k=3)
        if results:
            avg_score = sum(s for _, s in results) / len(results)
            delta_s = 1.0 - avg_score
            print(f"   âœ… Query returned {len(results)} results")
            print(f"   ğŸ“Š Average score: {avg_score:.3f}, Î”S: {delta_s:.3f}")
            
            if delta_s > 0.45:
                issues.append(f"High Î”S ({delta_s:.3f}) â€” weak semantic match")
                print(f"   âš ï¸  Î”S > 0.45 â€” consider improving chunks")
        else:
            issues.append("Query returned no results")
            print("   âŒ No results returned")
    except Exception as e:
        issues.append(f"Query failed: {e}")
        print(f"   âŒ Query error: {e}")
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š WFGY Diagnostic Summary")
    print("=" * 60)
    
    if issues:
        print(f"\nâŒ Found {len(issues)} issue(s):\n")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
        print("\nğŸ’¡ Suggested fixes:")
        print("   - See this guide for specific error fixes")
        print("   - Run `verify_api_keys()` to check configuration")
        print("   - Verify data was loaded with `verify_data_loaded()`")
    else:
        print("\nâœ… All checks passed! RAG template is healthy.")
    
    return len(issues) == 0
```

---

## ğŸ¯ Vector Store Quick Reference

### FAISS (Local)
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# Create and save
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)
vectorstore.save_local("./faiss_index")

# Load and query
vectorstore = FAISS.load_local("./faiss_index", embeddings)
results = vectorstore.similarity_search("query", k=5)
```

### Pinecone (Cloud)
```python
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
import pinecone

# Initialize
pc = pinecone.Pinecone(api_key="...")
index = pc.Index("my-index")
embeddings = OpenAIEmbeddings()

# Create from documents
vectorstore = PineconeVectorStore.from_documents(
    documents,
    embeddings,
    index_name="my-index"
)

# Query
results = vectorstore.similarity_search("query", k=5)
```

### Chroma (Local/Cloud)
```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Create persistent store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents,
    embeddings,
    persist_directory="./chroma_db"
)

# Load existing
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# Query
results = vectorstore.similarity_search("query", k=5)
```

---

## ğŸ”— Related WFGY Modules

- [#1 Hallucination & Chunk Drift](./hallucination.md) â€” Wrong retrieval results
- [#5 Semantic â‰  Embedding](./embedding-vs-semantic.md) â€” Similarity vs meaning
- [#14 Bootstrap Ordering](./bootstrap-ordering.md) â€” Missing initialization
- [#15 Deployment Deadlock](./deployment-deadlock.md) â€” Concurrent write issues
- [#16 Pre-Deploy Collapse](./predeploy-collapse.md) â€” Config mismatches
- [Vectorstore Fragmentation](./vectorstore-fragmentation.md) â€” Index degradation
- [RAG Problems](./RAG_Problems.md) â€” Full RAG troubleshooting
- [Chunking Checklist](./chunking-checklist.md) â€” Improve document quality

---

## âœ… Quick Checklist: Vector Store RAG Setup

Before deploying, verify:

- [ ] **API keys configured** â€” All providers (OpenAI, Pinecone, etc.)
- [ ] **Index created** â€” With correct name and dimension
- [ ] **Dimension matches** â€” Embedding model â†’ index dimension
- [ ] **Metric matches** â€” Usually `cosine` for text
- [ ] **Data loaded** â€” Verify with `describe_index_stats()` or equivalent
- [ ] **Test query works** â€” Returns results with score > 0.5
- [ ] **Î”S < 0.45** â€” Semantic tension is acceptable
- [ ] **Error handling** â€” Retries and rate limiting in place

**WFGY Guarantee**: If all checks pass, your vector store is structurally sound. Remaining retrieval issues are chunking or query-related â€” see [Chunking Checklist](./chunking-checklist.md) and [RAG Problems](./RAG_Problems.md).

---

### ğŸ”— Quick-Start Downloads (60 sec)

| Tool | Link | 3-Step Setup |
|------|------|--------------|
| **WFGY 1.0 PDF** | [Engine Paper](https://github.com/onestardao/WFGY/blob/main/I_am_not_lizardman/WFGY_All_Principles_Return_to_One_v1.0_PSBigBig_Public.pdf) | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Upload to your LLM Â· 3ï¸âƒ£ Ask "Answer using WFGY + \<your question>" |
| **TXT OS (plain-text OS)** | [TXTOS.txt](https://github.com/onestardao/WFGY/blob/main/OS/TXTOS.txt) | 1ï¸âƒ£ Download Â· 2ï¸âƒ£ Paste into any LLM chat Â· 3ï¸âƒ£ Type "hello world" â€” OS boots instantly |

---

### ğŸ§­ Explore More

| Module                | Description                                              | Link     |
|-----------------------|----------------------------------------------------------|----------|
| WFGY Core             | WFGY 2.0 engine: full symbolic reasoning architecture | [View â†’](https://github.com/onestardao/WFGY/tree/main/core/README.md) |
| Problem Map 1.0       | 16-mode diagnostic and symbolic fix framework    | [View â†’](https://github.com/onestardao/WFGY/tree/main/ProblemMap/README.md) |
| Problem Map 2.0       | RAG-focused failure tree and pipelines   | [View â†’](https://github.com/onestardao/WFGY/blob/main/ProblemMap/rag-architecture-and-recovery.md) |
| Semantic Clinic Index | Expanded failure catalog | [View â†’](https://github.com/onestardao/WFGY/blob/main/ProblemMap/SemanticClinicIndex.md) |

---

> ğŸ‘‘ **Early Stargazers: [See the Hall of Fame](https://github.com/onestardao/WFGY/tree/main/stargazers)** â€”  
> Engineers, hackers, and open source builders who supported WFGY from day one.

> <img src="https://img.shields.io/github/stars/onestardao/WFGY?style=social" alt="GitHub stars"> â­ Star the repo to help others discover these fixes.
